function crp_gibbs(datas,iter,class_id,pc_max_ind,pc_gammaln_by_2,pc_log_pi,pc_log,N,k_0,mu_0,v_0,lambda_0,D,sums_squares,yyT,means,inv_cov,log_det_cov,counts,K_plus)

class_id=class_id[:,iter]

    for i=randperm(N)'

    if iter==1 & i==1
        continue
    end
       
        y = datas[:,i];
        
        old_class_id= class_id[i];
        
        if old_class_id != 0
            counts[old_class_id] = counts[old_class_id] -1;
            
            if counts[old_class_id]==0
                # delete class compact all data structures
                
                hits = class_id.>=old_class_id;
                class_id[hits] = class_id[hits]-1;
                K_plus = K_plus-1;
                
                hits = [1:old_class_id-1 old_class_id+1:(K_plus+1)];

                if !isempty(hits)
                means[:,1:K_plus] = means[:,hits];
                means[:,K_plus+1] = 0;
                sum_squares[:,:,1:K_plus] = sum_squares[:,:,hits];
                sum_squares[:,:,1+K_plus] = 0;
                counts[1:K_plus] = counts[hits];
                counts[K_plus+1] = 0;
                
                log_det_cov[1:K_plus] = log_det_cov[hits];
                log_det_cov[K_plus+1] = 0;
                inv_cov[:,:,1:K_plus] = inv_cov[:,:,hits];
                inv_cov[:,:,K_plus+1] = 0;
                end
                
            else
                means[:,old_class_id] = (1/(counts[old_class_id]))*((counts[old_class_id]+1)*means[:,old_class_id] - y);
                sum_squares[:,:,old_class_id] = sum_squares[:,:,old_class_id] - yyT[:,:,i];
            end
        end
        
        # complete the CRP prior with new source prob.
        if iter != 1
            prior = [counts[1:K_plus]; alpha]/(N-1+alpha)
        else
            prior = [counts[1:K_plus]; alpha]/(i-1+alpha)
        end
        
        likelihood = zeros(Float64,(length(prior),1))
        
        # as per Radford's Alg. 3 compute the posterior predictive
        # probabilities in two scenerios, 1) we will evaluate the
        # likelihood of sitting at all of the existing sources by computing
        # the probability of the datapoint under the posterior predictive
        # distribution with all points sitting at that source considered and
        # 2) we will compute the likelihood of the point under the
        # posterior predictive distribution with no observations
        
        for ell = 1:K_plus
            # get the class ids of the points sitting at source l
            n = counts[ell]
            
            m_Y = means[:,ell]
            mu_n = k_0/(k_0+n)*mu_0 + n/(k_0+n)*m_Y
            k_n = k_0+n
            v_n = v_0+n
            
            # set up variables for Gelman's formulation of the Student T
            # distribution
            v = v_n-D+1
            mu = mu_n
            
            
            # if old_class_id == ell means that this point used to sit at
            # source ell, all of the sufficient statistics have been updated
            # in sum_squares, counts, and means but that means that we have
            # to recompute log_det_Sigma and inv_Sigma.  if we reseat the
            # particle at its old source then we can put the old
            # log_det_Sigma and inv_Sigma back, otherwise we need to update
            # both the old source and the new source
            if old_class_id != 0
                if old_class_id .== ell
                    S = (sum_squares[:,:,ell] - n*(m_Y*m_Y'))
                    zm_Y = m_Y-mu_0
                    lambda_n = lambda_0 + S  + k_0*n/(k_0+n)*(zm_Y)*(zm_Y)'
                    Sigma = (lambda_n*(k_n+1)/(k_n*(v_n-D+1)))'
                    
                    old_class_log_det_Sigma = log_det_cov[old_class_id]
                    old_class_inv_Sigma = inv_cov[:,:,old_class_id]
                    
                    log_det_Sigma = log(det(Sigma))
                    inv_Sigma = (Sigma)^-1
                    log_det_cov[old_class_id] = log_det_Sigma
                    inv_cov[:,:,old_class_id] = inv_Sigma
                else
                    log_det_Sigma = log_det_cov[ell]
                    inv_Sigma = inv_cov[:,:,ell];
                end
            else
                # this case is the first sweep through the data
                S = (sum_squares[:,:,ell] - n*(m_Y*m_Y'))
                zm_Y = m_Y-mu_0
                lambda_n = lambda_0 + S  +  k_0*n/(k_0+n)*(zm_Y)*(zm_Y)'
                Sigma = (lambda_n*(k_n+1)/(k_n*(v_n-D+1)))';
                
                log_det_Sigma = log(det(Sigma));
                inv_Sigma = (Sigma)^-1;
                log_det_cov[ell] = log_det_Sigma;
                inv_cov[:,:,ell] = inv_Sigma;
            end
            
            vd = v+D;
            
            # the log likelihood for class ell
            likelihood[ell] = (pc_gammaln_by_2[vd] - (pc_gammaln_by_2[v] + d2*pc_log[v] + d2*pc_log_pi) - .5*log_det_Sigma- (vd/2)*log(1+(1/v)*(y-mu)'*inv_Sigma*(y-mu)))[1]
            
        end
        
        likelihood[K_plus+1] = p_under_prior_alone[i];
        
        likelihood = exp(likelihood-max(likelihood));
        likelihood = likelihood/sum(likelihood);
        
        # compute the posterior over seating assignment for datum i
        posterior = prior.*likelihood; # this is actually a proportionality
        # normalize the posterior
        posterior = posterior/sum(posterior);
        
        # pick the new source
        cdf = cumsum(posterior);
        rn = rand()
        
        new_class_id = find(cdf.>rn)[1]
        
        counts[new_class_id] = counts[new_class_id]+1;
        means[:,new_class_id] = means[:,new_class_id]+ (1/counts[new_class_id])*(y-means[:,new_class_id]);
        sum_squares[:,:,new_class_id] = sum_squares[:,:,new_class_id] + yyT[:,:,i];
        
        if new_class_id .== (K_plus+1)
           K_plus = K_plus+1;
        end
        
        if old_class_id .== new_class_id
            # we don't need to compute anything new as the point was
            # already sitting at that source and the matrix inverse won't
            # change
            log_det_cov[old_class_id] = old_class_log_det_Sigma;
            inv_cov[:,:,old_class_id] = old_class_inv_Sigma;
        else
            # the point changed sources which means that the matrix inverse
            # sitting in the old_class_id slot is appropriate but that the
            # new source matrix inverse needs to be updated
            n = counts[new_class_id];
            #             if n~=0
            m_Y = means[:,new_class_id];
            k_n = k_0+n;
            v_n = v_0+n;
            
            # set up variables for Gelman's formulation of the Student T
            # distribution
            S = (sum_squares[:,:,new_class_id] - n*(m_Y*m_Y'));
            zm_Y = m_Y-mu_0;
            lambda_n = lambda_0 + S  + k_0*n/(k_0+n)*(zm_Y)*(zm_Y)';
            Sigma = (lambda_n*(k_n+1)/(k_n*(v_n-D+1)))';
            
            log_det_cov[new_class_id] = log(det(Sigma));
            inv_cov[:,:,new_class_id] = Sigma^-1;
        end
        
        # record the new source
        class_id[i] = new_class_id;
        
    end
    return(class_id,K_plus,sum_squares,means,inv_cov,log_det_cov,counts)
    
    end
